---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(BFDA)
# library(BayesFactor)
# library(MASS)
```

Exercises and Scripts for Trial Design part of Workshop: "Innovative approaches to clinical trial design and analysis", Ruhr-Universität Bochum, 19.11.2022

# Packages needed

You will need to install and load the following packages for these exercises:

```{r thepackages, eval = FALSE}

# install.packages("BayesFactor")
# install.packages("MASS")
# install.packages("devtools") # if not yet installed
# library(devtools)
# install_github("nicebread/BFDA", subdir="package")
# 
# library(BFDA)
# library(BayesFactor)
# library(MASS)

#Plus BFDA - see next section
```

```{r message=FALSE, warning=FALSE}
# install.packages("ggplot2")
# install.packages("lme4")
# install.packages("lmerTest")
# install.packages("dendextend")
# install.packages("tidyverse")
# install.packages("lcmm")
# install.packages("LCTMtools")
# install.packages("cowplot")
# install.packages("emmeans")
# install.packages("psych")
# install.packages("BBmisc")

library(ggplot2)
library(lme4)
library(lmerTest)
library(dendextend)
library(tidyverse)
library(lcmm)
library(LCTMtools)
library(cowplot)
library(emmeans)
library(psych)
library(BBmisc)
```

# Trial Design

## Exercise 1: Simulating data using BFDA and selecting trial parameters

The Bayes factor design analysis (BFDA) package provides a simple way to simulate and analyse sequential bayesian analyses for trial planning. For more complex analyses you can create your own custom simulations (e.g. see the scripts at https://osf.io/8mxda/ for example)

Please see https://rawgit.com/nicebread/BFDA/master/package/doc/BFDA_manual.html for the BFDA manual and full instructions

### Simulate sequential analyses for planning

A first step is to simulate the results of sequential analyses under i) the alternative hypothesis for the target effect size of interest (to find parameters that give you sufficient power), and ii) the null hypothesis (to find parameters that give a suitable false-positive (Type 1) error rate). Once you have suitable parameters for these, you can then simulate a range of other effect sizes to get a better picture of your power at different effect sizes.

As simulations can take some time, for demonstration purposes in this exercise we will i) attempt to find a medium effect size (d = 0.5), as this requires smaller maximum sample sizes, and ii) only do a small number of simulations (500), as we are not too worried about accuracy/precision - for planning an actual trial you would want to do a larger number (e.g. 10,000)

Please run the following simulations as preparation:

```{r BFDAsims, eval = FALSE}
set.seed(19112022)

sim.H1 <- BFDA.sim(expected.ES=0.5, type="t.between",prior=list("Cauchy",list(prior.location=0, prior.scale=sqrt(2)/2)), n.min=10, n.max=80, alternative="greater", boundary=Inf, B=500, verbose=TRUE, stepsize = 5)

sim.H0 <- BFDA.sim(expected.ES=0, type="t.between", prior=list("Cauchy", list(prior.location=0, prior.scale=sqrt(2)/2)), n.min=10, n.max=80, alternative="greater", boundary=Inf, B=500, verbose=TRUE, stepsize = 5)

#if you notice these have finished and you have time you could also run some in-between or larger values, e.g.

sim.H0.2 <- BFDA.sim(expected.ES=0.2, type="t.between", prior=list("Cauchy", list(prior.location=0, prior.scale=sqrt(2)/2)), n.min=10, n.max=80, alternative="greater", boundary=Inf, B=500, verbose=TRUE, stepsize = 5)

sim.H0.4 <- BFDA.sim(expected.ES=0.4, type="t.between", prior=list("Cauchy", list(prior.location=0, prior.scale=sqrt(2)/2)), n.min=10, n.max=80, alternative="greater", boundary=Inf, B=500, verbose=TRUE, stepsize = 5)

sim.H0.6 <- BFDA.sim(expected.ES=0.6, type="t.between", prior=list("Cauchy", list(prior.location=0, prior.scale=sqrt(2)/2)), n.min=10, n.max=80, alternative="greater", boundary=Inf, B=500, verbose=TRUE, stepsize = 5)

#etc...

```

As an explanation, these simulate sequential Bayes-factor based analyses based on:

a) Effect size of d = 0.5 (expected.ES=0.5) for sim.H1, or d=0 for sim.H0

b) a between-group t-test, e.g. difference between two groups in change in symptoms (type="t.between")

c) use a default Cauchy prior with a rscale value of 0.707 (prior=list("Cauchy", list(prior.location=0, prior.scale=sqrt(2)/2))) [we can't go into priors today but default ones like this work fine! See Stefan et al. (2019) for discussion of using informed priors with BFDA, https://link.springer.com/article/10.3758/s13428-018-01189-8]

d) a minimum sample size (per arm) of 10 (n.min = 10) - note that when trying to find parameters later you can choose higher minimum sample sizes, but not lower

e) a maximum sample size (per arm) of 80 (n.max = 80) - note that when trying out parameters later on you can try lower, but not higher maximum sample sizes

f) a directional (i.e. one-tailed) Bayes Factor (alternative = "greater" and boundary = Inf). We are interested in whether one treatment is superior to another, not whether it is different.

g) we use 500 simulations (B = 500) - fine for getting a feel, but for finalising your study design you would want a larger number e.g. 10,000

h) we carry out the analysis every 5 participants (stepsize = 5). You can repeat the analysis every 1 participant, but this simply takes longer to simulate so we don't do this here.

If you were carrying out large important simulations that took a long time you would probably want to save them so that you can load them again later without having to re-run the simulations:

```{r saveloaddemo, eval = FALSE}
#Saving a BFDA simulation object:
saveRDS(sim.H1,"sim.H1.d0.5.dd.mm.yy.RDS")
saveRDS(sim.H0,"sim.H1.d0.0.dd.mm.yy.RDS")
#to load again
sim.H1<-readRDS("sim.H1.d0.5.dd.mm.yy.RDS")
sim.H0<-readRDS("sim.H1.d0.0.dd.mm.yy.RDS")
```

### Try out different analysis parameters to find a set that fits your requirements

#### Overview

Now that you have a simulated set of sequential BFs you can test out what would happen if you applied different sets of analysis parameters to them.

These are:

* Nmin: The minimum sample size (per arm) at which you start the sequential analyses

* Nmax: The maximum sample size (per arm) at which you drop an arm (if it has not already hit a BF boundary)

* BFfail: A BF threshold for failure (i.e. sufficient evidence for the null hypothesis of non-superiority to the control condition vs. the alternative hypothesis of superiority). This will be a value less than 1 (e.g. 1/3, 1/5, 1/10 etc)

* BFsuccess: A BF threshold for success (i.e. sufficient evidence for the alternative hypothesis of superiority over the control condition vs.the null hypothesis of non-superiority to the control condition ). This will be a value greater than 1 (e.g. 3, 5, 10)

We are interested in:

i) power: what proportion of arms hit the BFsuccess threshold when d > 0

ii) false-positive / Type 1 error rate: what proportion of arms hit BF success threshold when d = 0 (or d < 0)

iii) keeping the average sample sizes as low as possible

There are several functions in BFDA to analyse simulation outcomes, but here we will just use the plot function. We will start by using the boundary conditions of the simulations (i.e. n.min = 10 and n.max = 80), and a default set of starting boundaries of BFfail = 1/5 and BFsuccess = 5 (boundary=c(1/5, 5)). The parameter n.trajectories just tells it how many lines to draw on the graph (representing individual BF trajectories)

```{r exampleplots, eval = FALSE}
#First plot for H1
dev.new() #sometimes the plot doesn't work if you don't make a new window first
plot(sim.H1, n.min=10, n.max=80, boundary=c(1/5, 5), n.trajectories = 60)

#Then plot for H0
dev.new() #sometimes the plot doesn't work if you don't make a new window first
plot(sim.H0, n.min=10, n.max=80, boundary=c(1/5, 5), n.trajectories = 60)

#There are other functions in BFDA to explore the simulations/sequential analyses but I find the plots the most informative. However, feel free to explore.
```

#### Exercise steps

1. Play around with the parameters (n.min, n.max, the BF boundaries) and see what happens. e.g. increasing n.min will tend to reduce error rates, but means you lose the chance to make decisions so quickly. Remember that the BF boundaries don't have to be symmetrical (e.g. you could use boundary=c(1/3,10) if you weren't so worried about false-negatives but were very concerned about potential false-positives)

2. Try to come up with a set of parameters that gives you 80% power and a false-positive (type 1) error rate of <5%, i.e. for H1, $\geq$ 80% stopping at H1 boundary, for H0, < 5% (i.e. 4%) stopping at h1 boundary. 

3. Once you find this you can see if you can improve on these, to reduce the potential sample sizes needed.

4. See what proportion of participants will hit BFsuccess (H1 boundary) for sim.H1 at different sample sizes (by adjusting n.max): the value for which 50% are stopping at the nmax boundary tells you the average sample size you might expect if d=0.5 with these parameters. You can then do the same for sim.H0. It might be that you can find a set of parameters that give you smaller average sample size predictions.

5. If you have simulated other effect sizes you can see what happens for these, e.g. what if you power for d=0.5, but d=0.4? 

#### Summarising the simulations for planning

One way to collect this information is into a table, as provided in the examples in Table 2 and Table 3 in the paper by Blackwell et al. (2019) https://journals.sagepub.com/doi/full/10.1177/2167702619858071

Table 2 from the paper is reproduced below. This illustrates a particular set of parameters chosen for a small to medium between-group effect size equivalent to Cohen’s d = 0.4: Nmin = 35 (per arm), Nmax = 125 (per arm), BFfail = 1/4, and a BFsuccess = 5 (and directional default Cauchy prior, rscale parameter = sqrt2/2).

Here we can see that we have a false-positive rate of < 5% (top row), and 81% to find d = 0.4. We can also see that 54% of the time, we would hit BFfail at Nmin when d = 0, and 54% of the time we would stop the trial at n = 50 per arm when d = 0.4 (8% at BFfail, 46% at BFsuccess):

<br><table><thead style="background-color: lightgray; border: 1px solid black;"><tr><th data-xml-align="left" rowspan="3">“True” effect size (Cohen’s <i>d</i>)</th><th data-xml-align="center" colspan="10"><span>Probability of reaching threshold at each participant number (per group)</span></th></tr><tr data-xml-align="center"><th colspan="5"><span>Discontinuation threshold</span></th><th colspan="5"><span>Replacement threshold</span></th></tr><tr data-xml-align="center"><th><i>n</i> = 35</th><th><i>n</i> = 50</th><th><i>n</i> = 75</th><th><i>n</i> = 100</th><th><i>n</i> = 125</th><th><i>n</i> = 35</th><th><i>n</i> = 50</th><th><i>n</i> = 75</th><th><i>n</i> = 100</th><th><i>n</i> = 125</th></tr></thead><tbody><tr><td data-xml-align="left">0 (null)</td><td data-xml-align="center">54</td><td data-xml-align="center">70</td><td data-xml-align="center">81</td><td data-xml-align="center">86</td><td data-xml-align="center">89</td><td data-xml-align="center"><b>1</b></td><td data-xml-align="center"><b>3</b></td><td data-xml-align="center"><b>3</b></td><td data-xml-align="center"><b>4</b></td><td data-xml-align="center"><b>4</b></td></tr><tr><td data-xml-align="left">0.1</td><td data-xml-align="center">37</td><td data-xml-align="center">52</td><td data-xml-align="center">62</td><td data-xml-align="center">68</td><td data-xml-align="center">71</td><td data-xml-align="center">4</td><td data-xml-align="center">7</td><td data-xml-align="center">10</td><td data-xml-align="center">12</td><td data-xml-align="center"><i>13</i></td></tr><tr><td data-xml-align="left">0.2</td><td data-xml-align="center">22</td><td data-xml-align="center">33</td><td data-xml-align="center">41</td><td data-xml-align="center">45</td><td data-xml-align="center">47</td><td data-xml-align="center">8</td><td data-xml-align="center">15</td><td data-xml-align="center">23</td><td data-xml-align="center">28</td><td data-xml-align="center"><i>32</i></td></tr><tr><td data-xml-align="left">0.3</td><td data-xml-align="center">11</td><td data-xml-align="center">18</td><td data-xml-align="center">22</td><td data-xml-align="center">24</td><td data-xml-align="center">25</td><td data-xml-align="center">16</td><td data-xml-align="center">29</td><td data-xml-align="center">43</td><td data-xml-align="center">52</td><td data-xml-align="center"><i>58</i></td></tr><tr><td data-xml-align="left">0.4</td><td data-xml-align="center">5</td><td data-xml-align="center">8</td><td data-xml-align="center">10</td><td data-xml-align="center">10</td><td data-xml-align="center">11</td><td data-xml-align="center">28</td><td data-xml-align="center">46</td><td data-xml-align="center">65</td><td data-xml-align="center">75</td><td data-xml-align="center"><i>81</i></td></tr><tr><td data-xml-align="left">0.5</td><td data-xml-align="center">2</td><td data-xml-align="center">3</td><td data-xml-align="center">4</td><td data-xml-align="center">4</td><td data-xml-align="center">4</td><td data-xml-align="center">43</td><td data-xml-align="center">64</td><td data-xml-align="center">82</td><td data-xml-align="center">91</td><td data-xml-align="center"><i>94</i></td></tr><tr><td data-xml-align="left">0.6</td><td data-xml-align="center">1</td><td data-xml-align="center">1</td><td data-xml-align="center">1</td><td data-xml-align="center">1</td><td data-xml-align="center">1</td><td data-xml-align="center">60</td><td data-xml-align="center">80</td><td data-xml-align="center">94</td><td data-xml-align="center">97</td><td data-xml-align="center"><i>98</i></td></tr><tr><td data-xml-align="left">0.7</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">75</td><td data-xml-align="center">91</td><td data-xml-align="center">98</td><td data-xml-align="center">100</td><td data-xml-align="center"><i>100</i></td></tr><tr><td data-xml-align="left">0.8</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">0</td><td data-xml-align="center">88</td><td data-xml-align="center">97</td><td data-xml-align="center">100</td><td data-xml-align="center">100</td><td data-xml-align="center"><i>100</i></td></tr></tbody></table><br><br>

Doing these plots is quite a long-winded way to arrive at such a table, but it at least gives you a good feel for what happens when you change the parameters. If you were doing your own simulations you could write a script to output a table automatically (e.g. see the scripts at https://osf.io/8mxda/ for example)

## Exercise 2: Carry out sequential analysis of (simulated) trial data

### Overview and setup

In this exercise you will analyse some (simulated) data using the trial parameters you selected, and find out when you hit the BF boundaries.

First you will need to simulate the trial data - please run the code below (which I'm happy to explain but there's no need to understand it for the exercise - it's just a means to provide you with the data to analyse):

```{r simtrialdata, eval = FALSE}
#Simulates 100 participants in each arm of trial where:
#There is a correlation of r = 0.35 between pre and post-data
#One arm is not superior to control, the other is
set.seed(19112022)
samples=100
r=0.35
data<-mvrnorm(n=samples,mu=c(1,0.8),Sigma=matrix(c(1,r,r,1),nrow=2),empirical=FALSE)
X = data[,1]
Y = data[,2]
X<-as.integer(X*5+20)
Y<-as.integer(Y*5+20)
data<-mvrnorm(n=samples,mu=c(1,0.15),Sigma=matrix(c(1,r,r,1),nrow=2),empirical=FALSE)
X1 = data[,1]
Y1 = data[,2]
X1<-as.integer(X1*5+20)
Y1<-as.integer(Y1*5+20)
data<-mvrnorm(n=samples,mu=c(1,0.9),Sigma=matrix(c(1,r,r,1),nrow=2),empirical=FALSE)
X2 = data[,1]
Y2 = data[,2]
X2<-as.integer(X2*5+20)
Y2<-as.integer(Y2*5+20)
cdiff<-Y-X
T1diff<-Y1-X1
T2diff<-Y2-X2
predata<-c(X,X1,X2)
postdata<-c(Y,Y1,Y2)
diffdata<-postdata-predata
group<-c(rep("C",samples),rep("Tx1",samples),rep("Tx2",samples))
dseq<-seq(1,samples*3)
pid<-as.character(seq(1,samples*3))
mydata<-(cbind(predata,postdata,diffdata,group))
mydata<-mydata[sample(1:nrow(mydata)), ]
mydata<-data.frame(cbind(pid,dseq,mydata))
mydata$dseq<-as.integer(mydata$dseq)
mydata$predata<-as.integer(mydata$predata)
mydata$postdata<-as.integer(mydata$postdata)
mydata$diffdata<-as.integer(mydata$diffdata)

```

You now have a dataframe (mydata) with pre and post outcome data for a pretend 3-arm trial of treatments for depression (i.e. a decrease in score on the outcome measure is good). There are three treatment arms, "C" (control condition, e.g. TA), "Tx1" (New treatment 1) and "Tx2" (New treatment 2). One arm is superior to control, one isn't. Each arm includes 100 participants. 

Dataframe columns are:

* pid = participant id (a string of a number from 1 to 300)
* dseq = sequence in which they provided outcome data for the trial (numerical, here the same as pid for convenience)
* predata = score on the depression outcome measure at pre-treatment
* postdata = score on the depression outcome measure at post-treatment
* diffdata = post-treatment score minus pre-treatment score
* group = which group (C, Tx1, Tx2)

### Exploring the data

Take a look at the data (e.g. with View(mydata) ) to get a sense of it. You can then use the functions in the code chunk below to explore the potential outcome of applying sequential Bayesian analyses. 

First, run the code below to load the two functions:

```{r BFsnapshot, eval = FALSE}

BFsnapshot<-function(BFdata,n=300,rs=(sqrt(2)/2)){

  if (length(BFdata$pid)<n){
    return("Error: n larger than number of participants");
  }
  
  tryCatch(
        {
          tdata<-BFdata[1:n,]
          cat("\n\n Total N = ",n,"\n\n")
          cat("Control: n = ",length(tdata[tdata$group=="C",]$pid),", mean change (post minus pre-treatment) = ", mean(tdata[tdata$group=="C",]$diffdata,na.rm=TRUE)," (SD = ", sd(tdata[tdata$group=="C",]$diffdata,na.rm=TRUE),")\n\n",sep="")
         cat("Tx1: n = ",length(tdata[tdata$group=="Tx1",]$pid),", mean change (post minus pre-treatment) = ", mean(tdata[tdata$group=="Tx1",]$diffdata,na.rm=TRUE)," (SD = ", sd(tdata[tdata$group=="Tx1",]$diffdata,na.rm=TRUE),")\n\n",sep="")
         #effect size calculation
         d<-effectsize::cohens_d(tdata[tdata$group=="C",]$diffdata,tdata[tdata$group=="Tx1",]$diffdata)
         cat("Effect size vs. control: d=",d$Cohens_d," 95% CIs [",d$CI_low,",",d$CI_high,"]\n\n",sep="")
         #Calculates directional Bayesian t-test (nullinterval=c(0,Inf))
         BF<-BayesFactor::ttestBF(x=tdata[tdata$group=="C",]$diffdata,y=tdata[tdata$group=="Tx1",]$diffdata,nullinterval=c(0,Inf),rscale=rs)
        cat("BF vs. control: BF=",exp(BF@bayesFactor$bf),"\n\n",sep="")

        cat("Tx2: n = ",length(tdata[tdata$group=="Tx2",]$pid),", mean change (post minus pre-treatment) = ", mean(tdata[tdata$group=="Tx2",]$diffdata,na.rm=TRUE)," (SD = ", sd(tdata[tdata$group=="Tx2",]$diffdata,na.rm=TRUE),")\n\n",sep="")
        #effect size calculation
         d<-effectsize::cohens_d(tdata[tdata$group=="C",]$diffdata,tdata[tdata$group=="Tx2",]$diffdata)
         cat("Effect size vs. control: d=",d$Cohens_d," 95% CIs [",d$CI_low,",",d$CI_high,"]\n\n",sep="")
         #Calculates directional Bayesian t-test (nullinterval=c(0,Inf))
         BF<-BayesFactor::ttestBF(x=tdata[tdata$group=="C",]$diffdata,y=tdata[tdata$group=="Tx2",]$diffdata,nullinterval=c(0,Inf),rscale=rs)
        cat("BF vs. control: BF=",exp(BF@bayesFactor$bf),"\n\n",sep="") 
          
        },
        error=function(cond) {
          return (paste0("Something went wrong! ",cond))
        })
}

seqBFs<-function(BFdata,rs=(sqrt(2)/2)){
  tryCatch(
        {
          alldata<-BFdata[1:length(BFdata$pid),]
          dsTx1<-vector()
          dsTx2<-vector()
          BFsTx1<-vector()
          BFsTx2<-vector()
          nsTx1<-vector()
          nsTx2<-vector()
          is<-vector()
          #start at index number 10 to avoid problems
          for (i in 10:length(alldata$pid)){
          tdata<-alldata[1:i,]
          Ns<-as.data.frame(cbind(table(tdata[1:i,]$group)))
          dsTx1<-c(dsTx1,(effectsize::cohens_d(tdata[tdata$group=="C",]$diffdata,tdata[tdata$group=="Tx1",]$diffdata))$Cohens_d)
          BFsTx1<-c(BFsTx1,exp((BayesFactor::ttestBF(x=tdata[tdata$group=="C",]$diffdata,y=tdata[tdata$group=="Tx1",]$diffdata,nullinterval=c(0,Inf),rscale=rs)@bayesFactor$bf)))
            nsTx1<-c(nsTx1,Ns["Tx1",])
            
            dsTx2<-c(dsTx2,(effectsize::cohens_d(tdata[tdata$group=="C",]$diffdata,tdata[tdata$group=="Tx2",]$diffdata))$Cohens_d)
            BFsTx2<-c(BFsTx2,exp((BayesFactor::ttestBF(x=tdata[tdata$group=="C",]$diffdata,y=tdata[tdata$group=="Tx2",]$diffdata,nullinterval=c(0,Inf),rscale=rs)@bayesFactor$bf)))
            nsTx2<-c(nsTx2,Ns["Tx2",])
            is<-c(is,i)
          
        }
          allresults<-data.frame(cbind(is,nsTx1,dsTx1,BFsTx1,nsTx2,dsTx2,BFsTx2))
          return(allresults)
        },
        error=function(cond) {
          return (paste0("Something went wrong! ",cond))
        })
}
```

You can now use these two functions to explore the effect of doing sequential Bayesian analyses (here for simplicity, t-test on change scores).

#### Function: BFsnapshot()

BFsnapshot() can be used to give you a snapshot of the data at a particular point in time. Arguments are:

* BFdata = the dataframe with the data (i.e. here it is mydata)
* n = the total sample size / sequence number (up to a max of 300) you want to take the snapshot at (e.g. if you put 100 in here, it will give you a snapshot of the analysis outcomes at the point there were 100 total participants in the trial)
* rs = you can ignore unless you want to try change the rscale parameter (sets to sqrt2/2 as default)

e.g. BFsnapshot(mydata,100) would give you analysis output when there are 100 people in the trial

#### Function: seqBFs()

seqBFs() outputs a dataframe with sequential BFs over the course of the simulated trial. You just pass it the dataframe (and can specify a rscale parameter if you want to change this)

trialresults<-seqBFs(mydata)

will give you a dataframe (trialresults), which you can then investigate (e.g. via View(trialresults), or you can try making some plots) to see how the BFs develop over time.

Columns in the dataframe are:

* is = index / total N participants in the trial
* nsTx1 = n in Tx1 arm
* dsTx1 = effect size (cohen's d) for Tx1 vs control
* BFsTx1 = Bayes Factor for Tx1 vs control
* nsTx2 = n in Tx2 arm
* dsTx2 = effect size (cohen's d) for Tx2 vs control
* BFsTx2 = Bayes Factor for Tx2 vs control

#### Exercise task:

See if you can work out what would have happened if you used your chosen trial parameters on this data set. When would you hit a BF boundary for each treatment arm (if at all)? 

# Re-analysis of RCT data

## Exercise 1: Symptom Clusters

### Simulate Practice Data

First let us simulate some practice data. The data are based on the items of the Hamilton-Anxiety-Rating Scale.

Initially we determine a sample size:

```{r}
n = 350 # number of individuals
```

Then the names of the items:

```{r}
hama_names = c("Anxious Mood",
               "Tension",
               "Fears",
               "Insomnia",
               "Concentration and Memory",
               "Depressed Mood",
               "General somatic symptoms: muscular",
               "General somatic symptoms: sensory",
               "Cardiovascular symptoms",
               "Respiratory symptoms",
               "Gastro-intestinal symptoms",
               "Genito-urinary symptoms",
               "Other autonomic symptoms")
```

And finally mean values and standard deviations as well as a plausible covariance structure:

```{r}
hama_means = c(2.9669421,
               2.8044077,
               2.4559229,
               2.4297521,
               1.3815427,
               1.3071625,
               1.6129477,
               1.5633609,
               1.4531680,
               1.0330579,
               1.7190083,
               0.6694215,
               1.6198347)

hama_means_post = c(2.9669421-2.3,
                    2.8044077-2.4,
                    2.4559229-2.2,
                    2.4297521-0.8,
                    1.3815427-0.7,
                    1.3071625-1.2,
                    1.6129477-0.5,
                    1.5633609-0.4,
                    1.4531680- 0.2,
                    1.0330579-0.4,
                    1.7190083-0.1,
                    0.6694215-0.1,
                    1.6198347-0.8)

hama_sds = c(0.7136179,
             0.8339568,
             1.1314254,
             1.2283532,
             1.2313786,
             1.2454069,
             1.2715661,
             1.2264810,
             1.2583008,
             1.2200659,
             1.2815280,
             1.0691029,
             1.1585213)

hama_cor = read.csv("https://raw.githubusercontent.com/stephangoerigk/DZP_Workshop_Slides/master/hamacor.csv")
hama_cor = as.matrix(hama_cor[,-1])
```

Now, let us simulate:

```{r}
# Create baseline data
set.seed(123)
data_cluster_bl = round(
  faux::rnorm_multi(n = n,
                    mu = hama_means,
                    sd = hama_sds,
                    r = hama_cor,
                    varnames = hama_names,
                    empirical = F), 2)
data_cluster_bl$id = row.names(data_cluster_bl)
data_cluster_bl$time = 0

# Create post-treatment data

data_cluster_post = round(
  faux::rnorm_multi(n = n,
                    mu = hama_means_post,
                    sd = hama_sds,
                    r = hama_cor,
                    varnames = hama_names,
                    empirical = F), 2)
data_cluster_post$id = row.names(data_cluster_post)
data_cluster_post$time = 1
```

Let us briefly look at the data:

```{r}
psych::describe(data_cluster_bl)
```

Usually we would go on and create a sum score by adding up all the items:

```{r}
data_cluster_bl$sum = rowSums(data_cluster_bl[, 1:13])
data_cluster_post$sum = rowSums(data_cluster_post[, 1:13])
```

Now let us combine the datasets:

```{r}
data_cluster = rbind(data_cluster_bl, data_cluster_post)
```

### Traditional Approach

Plot the data as we usually would using an average group trajectory:

```{r}
ggplot(data = data_cluster, aes(x = time, y = sum)) +
  stat_summary(geom = "line", fun = "mean") +
   stat_summary(geom = "point", fun = "mean") +
  scale_x_continuous(breaks = c(0,1)) +
  labs(y = "HAM-A sum", x = "Time") +
  theme_classic()
```

Analyze using LMM:

```{r}
summary(lmer(sum ~ time + (1|id), data = data_cluster))
```

### Prepare for Clustering

Let us see if we can find some item clusters in the baseline data to get a more differentiated picture:

For this we first need to drop the id, sum, and time columns:

```{r}
data_cluster_bl = BBmisc::dropNamed(data_cluster_bl, drop = c("time", "id", "sum"))
data_cluster_post = BBmisc::dropNamed(data_cluster_post, drop = c("time", "id", "sum"))
```

As a first step, it makes sense to scale the data, as not all item formats are identical in all scales:

```{r}
data_cluster_bl_s = scale(data_cluster_bl)
```

Next, we will transpose the data, since we want to cluster items into people and not vice versa:

```{r}
data_transposed = t(na.omit(data_cluster_bl_s))
```

Now we will creat a distance matrix to determine the proximity between item responses. The euclidean distance is a commonly used measure for psychometric measures (another one is the manhattan distance).

```{r}
d = dist(data_transposed, method = "euclidean")
```

And now, let us cluster the data. We use the `ward.D2` method, this way distances are squared ahead of clustering (no problem with negative data):

```{r}
clust = hclust(d, method = "ward.D2")
```

After the clustering is finished we should inspect the result. A common way to look at clustering solutions is the dendrogram:

```{r}
dend <- as.dendrogram(clust, hang = -1)

labels_cex(dend) = 2
marg = c(4, 4, 10, 35)
par(mar = marg, font = 1, cex = 0.4, cex.axis = 1.7, cex.lab = 2)
plot(rev(dend), horiz = T, edgePar = list(lwd = 2))
```

The earlier two items merge in the dendrogram, the more similar they were scored by the patients. We now have a good idea, which items belong together. However, now we need to decide how many clusters to retain, i.e. where to "cut" our dendrogram. This step is called pruning.

We will use the `cutreeDynamic()` function from the `dynamicTreeCut` package. It has many advantages over traditional methods (e.g. gap statistic, silhouette method) including that it is more sensitive for detection of distinct classes and more stable in bootstrapping procedures.

The argument `minClusterSize` should be set to 1 and the method should be `"hybrid"`.

```{r}
pruned = dynamicTreeCut::cutreeDynamic(clust, distM = as.matrix(d), method = "hybrid", minClusterSize = 1)

pruned
```

The `pruned` object includes our final clustering solution (i.e. which item belongs to which cluster).

We should pass the same names to it, that we used for the items:

```{r}
names(pruned) = hama_names
pruned
```

Now we can plot our pruned dendrogram. We will indicate class membership using colours:

```{r}
labels_colors(dend) = pruned[c(clust$order)]
labels_cex(dend) = 2
marg = c(4, 4, 10, 35)
par(mar = marg, font = 1, cex = 0.4, cex.axis = 1.7, cex.lab = 2)
plot(rev(dend), horiz = T, edgePar = list(lwd = 2))
```

```{r}
data_cluster_bl$id = row.names(data_cluster_bl)
data_cluster_bl$time = 0
data_cluster_post$id = row.names(data_cluster_post)
data_cluster_post$time = 1
```

```{r}
data_cluster_bl$sum_c1 = rowSums(data_cluster_bl[which(pruned == 1)]) / length(which(pruned == 1))
data_cluster_bl$sum_c2 = rowSums(data_cluster_bl[which(pruned == 2)]) / length(which(pruned == 2))
data_cluster_bl$sum_c3 = rowSums(data_cluster_bl[which(pruned == 3)]) / length(which(pruned == 3))
data_cluster_bl$sum_c4 = rowSums(data_cluster_bl[which(pruned == 4)]) / length(which(pruned == 4))

data_cluster_post$sum_c1 = rowSums(data_cluster_post[which(pruned == 1)]) / length(which(pruned == 1))
data_cluster_post$sum_c2 = rowSums(data_cluster_post[which(pruned == 2)]) / length(which(pruned == 1))
data_cluster_post$sum_c3 = rowSums(data_cluster_post[which(pruned == 3)]) / length(which(pruned == 1))
data_cluster_post$sum_c4 = rowSums(data_cluster_post[which(pruned == 4)]) / length(which(pruned == 1))
```

```{r}
data_cluster = rbind(data_cluster_bl, data_cluster_post)
data_cluster_long = multilevel::make.univ(data_cluster, data_cluster[,grep("sum", names(data_cluster))], outname = "Symptoms")
data_cluster_long = rename(data_cluster_long, Cluster = TIME)
data_cluster_long$Cluster = data_cluster_long$Cluster +1
data_cluster_long$Cluster = factor(data_cluster_long$Cluster)
```

```{r}
ggplot(data = data_cluster_long, aes(x = time, y = Symptoms, colour = Cluster)) +
  stat_summary(geom = "line", fun = "mean") +
   stat_summary(geom = "point", fun = "mean") +
  scale_x_continuous(breaks = c(0,1)) +
  labs(y = "Clusterscore", x = "Time") +
  theme_classic()
```

Let us model change, but now as a function of cluster:

First let us check the omnibus test:

```{r}
mod = lmer(Symptoms ~ time * Cluster + (1|id), data = data_cluster_long)

anova(mod)
```

There is a significant Cluster x Time interaction. Let us probe the interaction effect using pairwise comparisons between the cluster-specific slopes:

```{r}
emmeans::emtrends(mod, specs = pairwise ~ Cluster, var = "time")
```

##  Exercise 2: Response Trajectories

### Simulate Practice Data

```{r}
n = 350 # number of individuals
t = 1:10   # number of time periods

df = expand.grid(t = 1:max(t),
                 id = 1:n)
df$group = c(rep("active", nrow(df)/2), rep("placebo", nrow(df)/2))

trajectory = c("Linear response",
               "Deteriorate",
               "Rev. U-shape",
               "Rapid response",
               "No change")

set.seed(123)
for(ch in unique(df$id)){

  if(df$group[df$id == ch][1] == "active"){
    df$trajectory[df$id == ch] = rep(sample(trajectory, size = 1, replace = T, prob = c(.5, .05, .2, .2, .05)), max(t))
  }
  if(df$group[df$id == ch][1] == "placebo"){
    df$trajectory[df$id == ch] = rep(sample(trajectory, size = 1, replace = T, prob = c(.2, .2, .1, .05, .45)), max(t))
  }

  if(df$trajectory[df$id == ch][1] == "No change"){
    df$y[df$id == ch] = 24 + 0*t  + rnorm(nrow(df[df$id == ch,]), 0, 3)
  }
  if(df$trajectory[df$id == ch][1] == "Rev. U-shape"){
    df$y[df$id == ch] = 24 + 8*t - 0.9*t^2 + rnorm(nrow(df[df$id == ch,]), 0, 3)
  }
  if(df$trajectory[df$id == ch][1] == "Linear response"){
    df$y[df$id == ch] = 24 - 1*t  + rnorm(nrow(df[df$id == ch,]), 0, 3)
  }
  if(df$trajectory[df$id == ch][1] == "Deteriorate"){
    df$y[df$id == ch] = 24 + 2*t  + rnorm(nrow(df[df$id == ch,]), 0, 3)
  }
  if(df$trajectory[df$id == ch][1] == "Rapid response"){
    df$y[df$id == ch] = 24 - 10 * log(t) +  rnorm(nrow(df[df$id == ch,]), 0, 3)
  }
}
```

### Inspect the Data

Plot the data as we usually would (one trajectory per group)

```{r}
ggplot(data = df, aes(x = t, y = y, colour = group)) +
  stat_summary(geom = "line", fun = "mean") +
  stat_summary(geom = "point", fun = "mean") +
  scale_x_continuous(breaks = t) +
  coord_cartesian(ylim = c(0,50)) +
  labs(x = "Time", colour = "Group") +
  theme_classic()
```

Analyze using linear mixed model:

```{r message=FALSE, warning=FALSE}
library(lme4)
library(lmerTest)

summary(lmer(y ~ t * group + (1|id), data = df))
```

Plot data on individual change groups (without the mixture model, we usually do not know these in advance):

```{r}
ggplot(data = df, aes(x = t, y = y, colour = trajectory)) +
  geom_point() +
  scale_x_continuous(breaks = t) +
  coord_cartesian(ylim = c(0,50)) +
  facet_grid(cols = vars(trajectory), rows = vars(group)) +
  labs(colour = "trajectory", x = "Time") +
  theme_classic()
```

### Create LCLMM

To identify reasonable grouping categories for these individually improving patients, we need to build a latent model.

Let us compute a growth mixtue model aka. latent class linear mixed models (LCLMM). We use the package `lcmm` for this.

* The `fixed` argument is a formula, as we know it from mixed models. We determine a polynomial here (usually quadratic or cubic as this is how most symptoms have been shown to change).

* The `mixture` argument specifies class-specific fixed effects. These are the change parameters the trajectories are defined on

* The `mixture` argument specifies a random argument as in the LMM. May be `~ 1` for random intercepts or `1 + t` for random intercepts and slopes.

* The `ng` argument specifies the number of classes to be extracted. We will learn in a second how the optimal number of classes can be determined.

* The `subject` argument specifies the nesting structure due to the repeated measurements.

```{r}
library(lcmm)
library(LCTMtools)

mi = lcmm::hlme(fixed = y ~ 1 + t + I(t^2),
           mixture = ~ 1 + t + I(t^2),
           random = ~ 1,
           ng = 5,
           data = df,
           subject = "id")
```

### Inspect the LCLMM Model

Let us check the mixture object `mi`:

```{r}
mi
```

We can see an overview over our selected parameters and that the model has converged fine. We also get a selection of goodness-of-fit statistics, that we could use for model selection.

We can inspect the model further: The `LCTMtoolkit()` function gives us a convenient print out for the quality of our model and also displays some benchmark for orientation

```{r}
LCTMtoolkit_total = LCTMtoolkit(mi)
```

The `postprob()` function displays posterior classifications (i.e. group membership frequencies) for all extracted classes.

Often we want to define a minimum cutoff for clinical relevance (e.g. min. 5% capture of all patients):

```{r}
postprob_total = lcmm::postprob(mi)
```

### Plot LCLMM Model Predictions

Create custom function for LCLMM plotting:

```{r}
plot_traj = function(mi, data, var.time){
  datnew   <- data.frame(t = seq(0, max(data[, var.time]), length = 100))
  plotpred <- lcmm::predictY(mi, datnew, var.time = var.time, draws = TRUE)

  frame_traj = as.data.frame(expand.grid(Time = plotpred$times$t,
                                         trajectory = unique(mi$pprob$class),
                                         pred = NA,
                                         upper = NA,
                                         lower = NA))

  for(traj in unique(frame_traj$trajectory)){
    for(i in 1:100){
      frame_traj$pred[frame_traj$trajectory == traj][i] = plotpred$pred[,which(grepl(paste0("^Ypred_class", as.character(traj)), colnames(plotpred$pred)))][i]
      frame_traj$upper[frame_traj$trajectory == traj][i] = plotpred$pred[,which(grepl(paste0("^lower.Ypred_class", as.character(traj)), colnames(plotpred$pred)))][i]
      frame_traj$lower[frame_traj$trajectory == traj][i] = plotpred$pred[,which(grepl(paste0("^upper.Ypred_class", as.character(traj)), colnames(plotpred$pred)))][i]
    }
  }
  frame_traj$trajectory = factor(frame_traj$trajectory)
  return(ggplot(data = frame_traj, aes(x = Time, y = pred, ymin = lower, ymax = upper)) +
    # geom_vline(xintercept = c(5, 10, 14, 17), linetype = "dotted") +
    geom_line(aes(colour = trajectory)) +
      labs(y = "Predicted") +
    geom_ribbon(aes(fill = trajectory), alpha = .2, linetype = "dotted") +
    theme_classic())
}
```

Plot the result:

```{r}
plot_traj(mi, df, "t")
```

Let us check the graph next to the empirical data:

```{r}
cowplot::plot_grid(ggplot(data = df, aes(x = t, y = y, colour = trajectory)) +
                     geom_point() +
                     scale_x_continuous(breaks = t) +
                     coord_cartesian(ylim = c(0,50)) +
                     facet_grid(cols = vars(trajectory), rows = vars(group)) +
                     labs(colour = "Trajectory", x = "Time") +
                     theme_classic(),
                   plot_traj(mi, df, "t"), nrow = 2)
```

### Transfer class membership to original dataset

Now we should transfer the determined class to our empirical dataset. Otherwise, we will not be able to run models using the trajectories. 

In addition, we extract the certainty, that each person was classified to a category with (the `pprob` variable in the `mi` object). Using these probability values we can weigh later models for categorization uncertainty.

Create custom function for transfer:

```{r}
transfer_class = function(data, mi){
  data$class = NA
  for(ch in unique(data$id)){
    data$class[data$id == ch] = mi$pprob$class[mi$pprob$id == ch]
    data$weight[data$id == ch] = mi$pprob[mi$pprob$id == ch, which(grepl(paste0("prob", as.character(mi$pprob$class[mi$pprob$id == ch])), colnames(mi$pprob)))]
  }
  data$class = factor(data$class)
  return(data)
}
```

Transfer data to our original dataframe `df`:

```{r}
df = transfer_class(data = df, mi = mi)
```

Create data in wide format:

```{r}
df_wide = as.data.frame(df %>%
  pivot_wider(names_from = t, values_from = y))

levels(df_wide$class) = c( "Rapid response",
                      "Linear response",
                      "No change",
                      "Rev. U-shape",
                      "Deteriorate")
```

### Modeling class membership as dependent variable

We can check the dispersion of the trajectory class variable within the 2 treatment groups using `table()`:

```{r}
table(df_wide$class, df_wide$group)
```

If we want to use trajectory class membership as the dependent variable, we need to used a logistic-regression model (because `class` is a categorical variable). Since we usually have more than 2 trajectory classes, we will use a multinomial logistic-regression model.

```{r}
multinom = nnet::multinom(class ~ group, data = df_wide, weights = weight)
```

For an omnibus test, we can use a chi-square-likelihood-ratio test:

```{r}
car::Anova(multinom)
```

For pairwise comparisons we can use the `emmeans` package:

```{r}
emmeans::lsmeans(multinom, pairwise ~ group | class, adjust="tukey", mode = "prob")
```

Estimates are displayed as log-odds, so we have to exponentiate them using `exp()` to get interpretable odds ratios (OR).

### Determine the optimal model

In the previous dataset, we have 

1. used a qudratic polynomial
2. extracted 5 groups 
3. allowed free variation of the intercept as the random effect.

However, usually we operate on a more data-driven approach, i.e. without knowing these parameters in advance.

There are several statistical criteria to determine a mode that optimally describes the data. For LCLMM, the most commonly used ones are:

* Bayesian Information Criterion (BIC)
  * lower values on these information criteria indicate better fitting models
  * models that do not fit better than the baseline model can be dismissed, and a selection of the best fitting models can be carried forward and examined further
* Entropy
  * ranges from 0.00 to 1.00
  * high values of entropy (> .80) indicate that individuals are classified with confidence
  * models with higher entropy are favored 
* Adjusted Lo-Mendell-Rubin likelihood ratio test 
  * corrected likelihood-ratio distribution (a chi-square distribution is inappropriate) to compare models with C and C – 1 unobserved groups
  * likelihood ratio tests compare models that differ only in the number of classes
  * significance test (p< .05) indicates that the model with C – 1 classes should be rejected in favor of the model with C classes

### Selection loop

To try this, let us run a model selection based on the Bayesian Information Criterion (BIC).

Since we have to test a lot of parameters, we run the model fitting procedure in a loop and save the results in a container:

This procedure can take quite a while, so we'll test it in a smaller dataset:

```{r}
test = read.csv("https://raw.githubusercontent.com/stephangoerigk/DZP_Workshop_Slides/master/test.csv")
```

Inspect the data:

```{r}
psych::describe(test)

head(test)
```

Let us first set up a container:

```{r}
results_total = data.frame(ng = NA,
                           Polynomial = NA,
                           Random = NA,
                           BIC = NA,
                           AIC = NA,
                           loglik = NA)
```

We will run out loop for 2:5 groups,quadratic vs. cubic polynomial and different random effect compositions:

```{r eval=FALSE}
set.seed(222)

for(ng in 2:5){
  for(random in c("~ 1", "~ 1 + t")){
    mi_sq <- lcmm::hlme(fixed = stress ~ 1 + t + I(t^2),
                        mixture = ~ 1 + t + I(t^2),
                        random = as.formula(random),
                        ng = ng,
                        nwg = FALSE,
                        idiag = FALSE,
                        data = test,
                        subject = "id")
    mi_cub <- lcmm::hlme(fixed = stress ~ 1 + t + I(t^3),
                         mixture = ~ 1 + t + I(t^3),
                         random = as.formula(random),
                         ng = ng,
                         nwg = FALSE,
                         idiag = FALSE,
                         data = test,
                         subject = "id")

    sq <- c(mi_sq$ng, 2, random, mi_sq$BIC, mi_sq$AIC, mi_sq$loglik)
    cub <- c(mi_cub$ng, 3, random, mi_cub$BIC, mi_cub$AIC, mi_cub$loglik)
    results_total = rbind(results_total, sq)
    results_total = rbind(results_total, cub)
  }
}

results_total = results_total[order(results_total$BIC, decreasing = T),]
```

The solution with the lowest BIC is chosen. Now we can once more fit the LCLMM, only now we used the determined parameters

CAVE: While a statistical determination of model parameters is important, there should always be clinical plausibility checks as well.

